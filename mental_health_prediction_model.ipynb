{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7a7b5f",
   "metadata": {},
   "source": [
    "# Multimodal Machine Learning for Mental Health Prediction in University Students\n",
    "\n",
    "This notebook implements a multimodal machine learning approach to predict mental health outcomes for university students, using three different algorithms:\n",
    "1. K-Nearest Neighbors (K-NN)\n",
    "2. Linear Regression\n",
    "3. Support Vector Machine (SVM)\n",
    "\n",
    "We'll process different types of data (multimodal approach) to create a comprehensive prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc6087",
   "metadata": {},
   "source": [
    "## 1. Required Libraries and Technologies\n",
    "\n",
    "Let's import all the necessary libraries for our multimodal machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f06b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix, r2_score\n",
    "\n",
    "# For feature selection and dimensionality reduction\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# For handling imbalanced datasets (if needed)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Display settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cdc51",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "For a mental health prediction model, we need to handle different types of data. In this section, we'll load and explore our dataset(s).\n",
    "\n",
    "### Data Types for Multimodal Approach:\n",
    "\n",
    "1. **Demographic data**: Age, gender, year of study, etc.\n",
    "2. **Academic data**: GPA, course load, etc.\n",
    "3. **Behavioral data**: Sleep patterns, exercise habits, social interactions\n",
    "4. **Psychological assessments**: Standardized mental health screening scores (e.g., PHQ-9 for depression, GAD-7 for anxiety)\n",
    "5. **External factors**: Financial stress, housing situation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f221dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for loading data\n",
    "# Replace this with actual data loading from your source\n",
    "\n",
    "# Option 1: Load data from a CSV file\n",
    "# df = pd.read_csv('student_mental_health_data.csv')\n",
    "\n",
    "# Option 2: Create sample data for demonstration\n",
    "np.random.seed(42)\n",
    "size = 500\n",
    "\n",
    "# Generate sample data\n",
    "data = {\n",
    "    # Demographics\n",
    "    'age': np.random.normal(20, 2, size).round(),\n",
    "    'gender': np.random.choice(['Male', 'Female', 'Non-binary'], size),\n",
    "    'year_of_study': np.random.choice([1, 2, 3, 4, 5], size),\n",
    "    \n",
    "    # Academic factors\n",
    "    'gpa': np.random.normal(3.0, 0.5, size).clip(0, 4.0),\n",
    "    'course_load': np.random.normal(5, 1, size).round().clip(2, 7),\n",
    "    'major': np.random.choice(['Engineering', 'Arts', 'Science', 'Business', 'Medicine'], size),\n",
    "    \n",
    "    # Behavioral data\n",
    "    'sleep_hours': np.random.normal(7, 1.5, size).clip(3, 10),\n",
    "    'exercise_hours_per_week': np.random.gamma(2, 1.5, size),\n",
    "    'social_activity_hours': np.random.gamma(3, 2, size),\n",
    "    \n",
    "    # Psychological assessments (simulated)\n",
    "    'depression_score': np.random.gamma(5, 1, size).round(),  # PHQ-9 like (0-27)\n",
    "    'anxiety_score': np.random.gamma(4, 1, size).round(),    # GAD-7 like (0-21)\n",
    "    'stress_score': np.random.gamma(6, 1, size).round(),     # PSS like\n",
    "    \n",
    "    # External factors\n",
    "    'financial_stress': np.random.choice([0, 1, 2, 3, 4], size),  # 0=None, 4=Severe\n",
    "    'housing_quality': np.random.choice([1, 2, 3, 4, 5], size),   # 1=Poor, 5=Excellent\n",
    "    'support_network': np.random.choice([0, 1, 2, 3, 4], size),   # 0=None, 4=Strong\n",
    "    \n",
    "    # Target variable: mental health index (could be classification or regression)\n",
    "    # For regression (continuous score)\n",
    "    'mental_health_index': np.random.normal(50, 15, size).clip(0, 100)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a categorical target for classification\n",
    "# Below 40: Poor mental health, 40-60: Average, Above 60: Good\n",
    "df['mental_health_category'] = pd.cut(\n",
    "    df['mental_health_index'], \n",
    "    bins=[0, 40, 60, 100], \n",
    "    labels=['Poor', 'Average', 'Good']\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db171d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()\n",
    "\n",
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the target variable (mental health index)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# For regression target\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['mental_health_index'], kde=True)\n",
    "plt.title('Distribution of Mental Health Index')\n",
    "\n",
    "# For classification target\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='mental_health_category', data=df)\n",
    "plt.title('Distribution of Mental Health Categories')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1584b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical features\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).drop(columns=['mental_health_index'])\n",
    "\n",
    "# Correlation with target\n",
    "correlations = numerical_features.corrwith(df['mental_health_index']).sort_values(ascending=False)\n",
    "\n",
    "# Plot correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=correlations.index, y=correlations.values)\n",
    "plt.title('Feature Correlation with Mental Health Index')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(14, 10))\n",
    "corr_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebbbf2",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Data Preprocessing\n",
    "\n",
    "For our multimodal approach, we need to properly process different types of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df.drop(columns=['mental_health_index', 'mental_health_category'])\n",
    "y_reg = df['mental_health_index']  # For regression tasks\n",
    "y_cls = df['mental_health_category']  # For classification tasks\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train_reg, y_test_reg, y_train_cls, y_test_cls = train_test_split(\n",
    "    X, y_reg, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(f\"Categorical features: {list(categorical_features)}\")\n",
    "print(f\"Numerical features: {list(numerical_features)}\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c890fb6",
   "metadata": {},
   "source": [
    "## 4. Building Machine Learning Models\n",
    "\n",
    "We'll implement three different models as specified:\n",
    "1. K-Nearest Neighbors (K-NN)\n",
    "2. Linear Regression / Logistic Regression\n",
    "3. Support Vector Machine (SVM)\n",
    "\n",
    "We'll create versions for both regression (predicting mental health index) and classification (predicting mental health category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. K-NN Models\n",
    "\n",
    "# For regression\n",
    "knn_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# For classification\n",
    "knn_cls_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Parameter grid for KNN\n",
    "knn_param_grid = {\n",
    "    'model__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Grid search for regression\n",
    "knn_reg_grid = GridSearchCV(\n",
    "    knn_reg_pipeline, \n",
    "    knn_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Grid search for classification\n",
    "knn_cls_grid = GridSearchCV(\n",
    "    knn_cls_pipeline, \n",
    "    knn_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train KNN models\n",
    "print(\"Training K-NN Regression model...\")\n",
    "knn_reg_grid.fit(X_train, y_train_reg)\n",
    "print(f\"Best parameters: {knn_reg_grid.best_params_}\")\n",
    "print(f\"Best RMSE: {(-knn_reg_grid.best_score_)**0.5:.4f}\")\n",
    "\n",
    "print(\"\\nTraining K-NN Classification model...\")\n",
    "knn_cls_grid.fit(X_train, y_train_cls)\n",
    "print(f\"Best parameters: {knn_cls_grid.best_params_}\")\n",
    "print(f\"Best accuracy: {knn_cls_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Linear Models\n",
    "\n",
    "# For regression\n",
    "linear_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# For classification\n",
    "logistic_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Parameter grid for logistic regression\n",
    "logistic_param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'model__solver': ['liblinear', 'saga'],\n",
    "    'model__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Train Linear Regression model\n",
    "print(\"Training Linear Regression model...\")\n",
    "linear_reg_pipeline.fit(X_train, y_train_reg)\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "linear_reg_pred = linear_reg_pipeline.predict(X_test)\n",
    "linear_reg_rmse = mean_squared_error(y_test_reg, linear_reg_pred, squared=False)\n",
    "linear_reg_r2 = r2_score(y_test_reg, linear_reg_pred)\n",
    "print(f\"Linear Regression RMSE: {linear_reg_rmse:.4f}\")\n",
    "print(f\"Linear Regression R²: {linear_reg_r2:.4f}\")\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "logistic_grid = GridSearchCV(\n",
    "    logistic_pipeline,\n",
    "    logistic_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train Logistic Regression model\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "logistic_grid.fit(X_train, y_train_cls)\n",
    "print(f\"Best parameters: {logistic_grid.best_params_}\")\n",
    "print(f\"Best accuracy: {logistic_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82010adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Support Vector Machine Models\n",
    "\n",
    "# For regression\n",
    "svm_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', SVR())\n",
    "])\n",
    "\n",
    "# For classification\n",
    "svm_cls_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', SVC(probability=True))\n",
    "])\n",
    "\n",
    "# Parameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    'model__C': [0.1, 1, 10],\n",
    "    'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'model__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Grid search for SVM regression\n",
    "svm_reg_grid = GridSearchCV(\n",
    "    svm_reg_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Grid search for SVM classification\n",
    "svm_cls_grid = GridSearchCV(\n",
    "    svm_cls_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train SVM models\n",
    "print(\"Training SVM Regression model...\")\n",
    "svm_reg_grid.fit(X_train, y_train_reg)\n",
    "print(f\"Best parameters: {svm_reg_grid.best_params_}\")\n",
    "print(f\"Best RMSE: {(-svm_reg_grid.best_score_)**0.5:.4f}\")\n",
    "\n",
    "print(\"\\nTraining SVM Classification model...\")\n",
    "svm_cls_grid.fit(X_train, y_train_cls)\n",
    "print(f\"Best parameters: {svm_cls_grid.best_params_}\")\n",
    "print(f\"Best accuracy: {svm_cls_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c6b1d",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison\n",
    "\n",
    "Let's evaluate all models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model evaluation\n",
    "def evaluate_regression_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Classification model evaluation\n",
    "def evaluate_classification_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Classification Report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Evaluate regression models\n",
    "reg_results = []\n",
    "reg_results.append(evaluate_regression_model(knn_reg_grid.best_estimator_, X_test, y_test_reg, \"K-NN\"))\n",
    "reg_results.append(evaluate_regression_model(linear_reg_pipeline, X_test, y_test_reg, \"Linear Regression\"))\n",
    "reg_results.append(evaluate_regression_model(svm_reg_grid.best_estimator_, X_test, y_test_reg, \"SVM\"))\n",
    "\n",
    "# Evaluate classification models\n",
    "cls_results = []\n",
    "cls_results.append(evaluate_classification_model(knn_cls_grid.best_estimator_, X_test, y_test_cls, \"K-NN\"))\n",
    "cls_results.append(evaluate_classification_model(logistic_grid.best_estimator_, X_test, y_test_cls, \"Logistic Regression\"))\n",
    "cls_results.append(evaluate_classification_model(svm_cls_grid.best_estimator_, X_test, y_test_cls, \"SVM\"))\n",
    "\n",
    "# Display results in a DataFrame\n",
    "reg_results_df = pd.DataFrame(reg_results).set_index('Model')\n",
    "print(\"Regression Models Performance:\")\n",
    "print(reg_results_df)\n",
    "\n",
    "# Display classification accuracy\n",
    "cls_accuracy_df = pd.DataFrame([{r['Model']: r['Accuracy'] for r in cls_results}]).T\n",
    "cls_accuracy_df.columns = ['Accuracy']\n",
    "print(\"\\nClassification Models Accuracy:\")\n",
    "print(cls_accuracy_df)\n",
    "\n",
    "# Display detailed classification reports\n",
    "for result in cls_results:\n",
    "    print(f\"\\n{result['Model']} Classification Report:\")\n",
    "    print(result['Classification Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression model performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=reg_results_df.index, y=reg_results_df['RMSE'])\n",
    "plt.title('Regression Models: RMSE Comparison (Lower is Better)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize classification model performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=cls_accuracy_df.index, y=cls_accuracy_df['Accuracy'])\n",
    "plt.title('Classification Models: Accuracy Comparison (Higher is Better)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c66a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression, we can analyze coefficients\n",
    "# First, get feature names after one-hot encoding\n",
    "cat_feature_names = list(linear_reg_pipeline.named_steps['preprocessor']\n",
    "                         .named_transformers_['cat']\n",
    "                         .named_steps['onehot']\n",
    "                         .get_feature_names_out(categorical_features))\n",
    "\n",
    "transformed_feature_names = list(numerical_features) + list(cat_feature_names)\n",
    "\n",
    "# Get coefficients from Linear Regression\n",
    "try:\n",
    "    coefficients = linear_reg_pipeline.named_steps['model'].coef_\n",
    "    coef_df = pd.DataFrame({'Feature': transformed_feature_names, 'Coefficient': coefficients})\n",
    "    coef_df = coef_df.sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    # Plot top 15 features by importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef_df.head(15))\n",
    "    plt.title('Top 15 Features by Importance (Linear Regression)')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't extract coefficients: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a000c6",
   "metadata": {},
   "source": [
    "## 6. Making Predictions on New Data\n",
    "\n",
    "Let's create a function to predict mental health outcomes for new students:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mental_health(student_data, reg_model=None, cls_model=None):\n",
    "    \"\"\"Predict mental health outcomes for a new student.\n",
    "    \n",
    "    Args:\n",
    "        student_data (dict): Dictionary containing student features\n",
    "        reg_model: Trained regression model\n",
    "        cls_model: Trained classification model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Predictions from both models\n",
    "    \"\"\"\n",
    "    # Convert input to DataFrame\n",
    "    student_df = pd.DataFrame([student_data])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Get regression prediction if model provided\n",
    "    if reg_model is not None:\n",
    "        mh_index = reg_model.predict(student_df)[0]\n",
    "        results['mental_health_index'] = mh_index\n",
    "    \n",
    "    # Get classification prediction if model provided\n",
    "    if cls_model is not None:\n",
    "        mh_category = cls_model.predict(student_df)[0]\n",
    "        category_probs = cls_model.predict_proba(student_df)[0]\n",
    "        results['mental_health_category'] = mh_category\n",
    "        results['category_probabilities'] = {cls_model.classes_[i]: category_probs[i] for i in range(len(cls_model.classes_))}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "# Select the best models based on evaluation\n",
    "best_reg_model = svm_reg_grid.best_estimator_ if svm_reg_grid.best_score_ > knn_reg_grid.best_score_ else knn_reg_grid.best_estimator_\n",
    "best_cls_model = svm_cls_grid.best_estimator_ if svm_cls_grid.best_score_ > knn_cls_grid.best_score_ else knn_cls_grid.best_estimator_\n",
    "\n",
    "# Example student data\n",
    "sample_student = {\n",
    "    'age': 21,\n",
    "    'gender': 'Female',\n",
    "    'year_of_study': 3,\n",
    "    'gpa': 3.7,\n",
    "    'course_load': 5,\n",
    "    'major': 'Engineering',\n",
    "    'sleep_hours': 6.5,\n",
    "    'exercise_hours_per_week': 2.5,\n",
    "    'social_activity_hours': 8.0,\n",
    "    'financial_stress': 2,\n",
    "    'housing_quality': 4,\n",
    "    'support_network': 3\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "prediction = predict_mental_health(sample_student, best_reg_model, best_cls_model)\n",
    "print(\"Mental Health Prediction for Sample Student:\")\n",
    "print(f\"Mental Health Index: {prediction.get('mental_health_index', 'N/A'):.2f}\")\n",
    "print(f\"Mental Health Category: {prediction.get('mental_health_category', 'N/A')}\")\n",
    "\n",
    "if 'category_probabilities' in prediction:\n",
    "    print(\"\\nProbability for each category:\")\n",
    "    for category, prob in prediction['category_probabilities'].items():\n",
    "        print(f\"{category}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84d343",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've implemented a multimodal machine learning approach to predict mental health outcomes for university students using three different algorithms: K-NN, Linear Regression/Logistic Regression, and SVM.\n",
    "\n",
    "### Key Accomplishments:\n",
    "- Created a data processing pipeline that handles both numerical and categorical data\n",
    "- Implemented and compared multiple machine learning models\n",
    "- Created a prediction function for new students\n",
    "\n",
    "### Next Steps:\n",
    "1. **Collect real data**: Replace the simulated data with real student data\n",
    "2. **Feature engineering**: Develop more sophisticated features\n",
    "3. **Advanced models**: Explore ensemble methods or deep learning approaches\n",
    "4. **User interface**: Develop a simple interface for university counselors\n",
    "5. **Ethical considerations**: Ensure privacy and proper use of predictions\n",
    "6. **Validation**: Validate the model with domain experts\n",
    "\n",
    "### Technologies Used:\n",
    "- **Data Processing**: NumPy, Pandas\n",
    "- **Machine Learning**: Scikit-learn\n",
    "- **Visualization**: Matplotlib, Seaborn, Plotly\n",
    "- **Statistical Analysis**: SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c867e6",
   "metadata": {},
   "source": [
    "## 8. Advanced Modeling Techniques\n",
    "\n",
    "Let's explore some advanced modeling techniques that could potentially improve our mental health prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ensemble methods\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# Create Ensemble Models\n",
    "print(\"Training Ensemble Models...\")\n",
    "\n",
    "# Random Forest models\n",
    "rf_reg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "rf_cls = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train Random Forest models\n",
    "rf_reg.fit(X_train, y_train_reg)\n",
    "rf_cls.fit(X_train, y_train_cls)\n",
    "\n",
    "# Evaluate Random Forest models\n",
    "rf_reg_results = evaluate_regression_model(rf_reg, X_test, y_test_reg, \"Random Forest\")\n",
    "rf_cls_results = evaluate_classification_model(rf_cls, X_test, y_test_cls, \"Random Forest\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRandom Forest Regression Performance:\")\n",
    "print(f\"RMSE: {rf_reg_results['RMSE']:.4f}, R²: {rf_reg_results['R²']:.4f}\")\n",
    "\n",
    "print(\"\\nRandom Forest Classification Performance:\")\n",
    "print(f\"Accuracy: {rf_cls_results['Accuracy']:.4f}\")\n",
    "print(rf_cls_results['Classification Report'])\n",
    "\n",
    "# Compare with previous models\n",
    "all_reg_results = pd.DataFrame(reg_results + [rf_reg_results]).set_index('Model')\n",
    "all_cls_results = pd.DataFrame([{r['Model']: r['Accuracy'] for r in cls_results}, {'Random Forest': rf_cls_results['Accuracy']}]).T\n",
    "all_cls_results.columns = ['Accuracy']\n",
    "\n",
    "# Plot updated comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=all_reg_results.index, y=all_reg_results['RMSE'])\n",
    "plt.title('All Regression Models: RMSE Comparison (Lower is Better)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot classification results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=all_cls_results.index, y=all_cls_results['Accuracy'])\n",
    "plt.title('All Classification Models: Accuracy Comparison (Higher is Better)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918228d2",
   "metadata": {},
   "source": [
    "## 9. Model Explainability with SHAP\n",
    "\n",
    "Let's use SHAP (SHapley Additive exPlanations) to better understand our model predictions. This helps in making our models more interpretable and transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ccce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if not already installed\n",
    "# !pip install shap\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's use the best classifier model for explanation\n",
    "best_classifier = rf_cls  # Using Random Forest as it typically works well with SHAP\n",
    "\n",
    "try:\n",
    "    # Create a function to get preprocessed data (only numerical for simplicity)\n",
    "    def get_preprocessed_data(X):\n",
    "        # Get only the numerical transformation part for simplicity\n",
    "        preprocessed_numerical = numerical_transformer.transform(X.select_dtypes(include=['int64', 'float64']))\n",
    "        return preprocessed_numerical\n",
    "    \n",
    "    # Get preprocessed training data\n",
    "    X_train_processed = get_preprocessed_data(X_train)\n",
    "    \n",
    "    # For demonstration, let's use the RandomForest's feature importance first\n",
    "    forest_model = rf_cls.named_steps['model']\n",
    "    importances = forest_model.feature_importances_\n",
    "    numerical_feature_names = list(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "    \n",
    "    # Create a DataFrame for feature importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': numerical_feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "    plt.title('Feature Importance from Random Forest')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Initialize SHAP explainer using the RandomForest model\n",
    "    explainer = shap.TreeExplainer(forest_model)\n",
    "    \n",
    "    # Calculate SHAP values - this can be computationally intensive\n",
    "    # Use a small subset for demonstration\n",
    "    sample_size = min(100, X_train_processed.shape[0])\n",
    "    X_sample = X_train_processed[:sample_size]\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Plot SHAP summary\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=numerical_feature_names)\n",
    "    plt.title('SHAP Summary Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot detailed SHAP values for a few samples\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    shap.plots.waterfall(explainer.expected_value[0], shap_values[0][0], feature_names=numerical_feature_names)\n",
    "    plt.title('SHAP Waterfall Plot for a Sample Prediction')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in SHAP analysis: {str(e)}\")\n",
    "    print(\"To use SHAP, you may need to install it with: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2043d0",
   "metadata": {},
   "source": [
    "## 10. Acquiring Real-World Mental Health Datasets\n",
    "\n",
    "The current model uses synthetic data generated for demonstration. For a production-ready system, you'll need real student mental health data. Here are some options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some potential sources of mental health datasets and approaches for data collection\n",
    "\n",
    "# 1. Public Mental Health Datasets\n",
    "'''\n",
    "Some available public datasets that could be used or adapted:\n",
    "\n",
    "1. Student Mental Health Dataset (Kaggle): https://www.kaggle.com/datasets/shariful07/student-mental-health\n",
    "   This dataset contains survey responses from university students about their mental health conditions.\n",
    "\n",
    "2. Depression, Anxiety and Stress Scale Responses (Kaggle): https://www.kaggle.com/datasets/lucasgreenwell/depression-anxiety-stress-scales-responses\n",
    "   Contains responses to the DASS (Depression, Anxiety and Stress Scale).\n",
    "\n",
    "3. Mental Health in Tech Survey: https://www.kaggle.com/datasets/osmi/mental-health-in-tech-survey\n",
    "   While focused on tech employees, the survey structure could be adapted for students.\n",
    "\n",
    "4. University Student Mental Health Survey with Demographics: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TZLULU\n",
    "'''\n",
    "\n",
    "# 2. Creating your own data collection protocol\n",
    "def create_mental_health_assessment_protocol():\n",
    "    '''\n",
    "    To create your own data collection protocol:\n",
    "    \n",
    "    1. Use standardized assessment tools:\n",
    "       - PHQ-9 for depression screening\n",
    "       - GAD-7 for anxiety screening\n",
    "       - PSS (Perceived Stress Scale) for stress levels\n",
    "       - AUDIT for alcohol use\n",
    "       - PSQI for sleep quality\n",
    "       \n",
    "    2. Collect demographic and academic information:\n",
    "       - Age, gender, year of study\n",
    "       - Major/program\n",
    "       - GPA and course load\n",
    "       - Living situation\n",
    "       - Financial status/concerns\n",
    "       \n",
    "    3. Additional data sources:\n",
    "       - Academic performance data (with permissions)\n",
    "       - Campus resource usage (counseling services)\n",
    "       - Wearable device data (sleep patterns, activity levels)\n",
    "       \n",
    "    4. Ensure ethical considerations:\n",
    "       - IRB/Ethics board approval\n",
    "       - Informed consent\n",
    "       - Data anonymization\n",
    "       - Secure data storage\n",
    "    '''\n",
    "    \n",
    "    print(\"Protocol for mental health data collection created.\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# 3. Sample code for loading a real dataset (example with Kaggle dataset)\n",
    "'''\n",
    "To use the Student Mental Health dataset from Kaggle:\n",
    "\n",
    "1. Download the dataset from Kaggle: https://www.kaggle.com/datasets/shariful07/student-mental-health\n",
    "2. Place the CSV file in your project directory\n",
    "3. Run the code below to load and preprocess it\n",
    "'''\n",
    "\n",
    "# Uncomment and modify this code to load a real dataset\n",
    "\n",
    "'''\n",
    "def load_real_student_mental_health_data(file_path=\"student_mental_health.csv\"):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Perform necessary preprocessing\n",
    "    # (This will depend on the specific dataset structure)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.dropna()  # Or use imputation techniques\n",
    "    \n",
    "    # Encode categorical variables as needed\n",
    "    # Example: df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n",
    "    \n",
    "    # Create target variables\n",
    "    # Example: df['mental_health_category'] = pd.cut(df['depression_score'], bins=[0, 5, 10, 27], labels=['Mild', 'Moderate', 'Severe'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the real dataset\n",
    "# real_df = load_real_student_mental_health_data()\n",
    "# Display the first few rows\n",
    "# real_df.head()\n",
    "'''\n",
    "\n",
    "# 4. Data collection considerations\n",
    "'''\n",
    "When collecting mental health data from students:\n",
    "\n",
    "1. Privacy and Security:\n",
    "   - Ensure HIPAA/FERPA compliance\n",
    "   - Use secure storage and transmission\n",
    "   - Implement proper access controls\n",
    "\n",
    "2. Ethical Considerations:\n",
    "   - Obtain informed consent\n",
    "   - Provide resources for students in distress\n",
    "   - Have protocols for high-risk cases\n",
    "   - Allow students to opt out or withdraw\n",
    "\n",
    "3. Data Quality:\n",
    "   - Use validated instruments\n",
    "   - Ensure representative sampling\n",
    "   - Consider longitudinal data collection\n",
    "   - Combine self-report with objective measures when possible\n",
    "\n",
    "4. Institutional Collaboration:\n",
    "   - Work with university health services\n",
    "   - Partner with psychology/psychiatry departments\n",
    "   - Engage student organizations\n",
    "'''\n",
    "\n",
    "print(\"To proceed with this project, you will need to acquire or collect real mental health data, following appropriate ethical and privacy guidelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853d310",
   "metadata": {},
   "source": [
    "## 11. Creating a Simple Web Application for University Counselors\n",
    "\n",
    "To deploy this model for practical use by university counseling services, we can create a simple web application using Streamlit. This interface will allow counselors to input student data and receive mental health predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0382987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to install Streamlit\n",
    "# !pip install streamlit\n",
    "\n",
    "'''\n",
    "Sample code for a Streamlit application to deploy the mental health prediction model:\n",
    "\n",
    "```python\n",
    "# Save this as app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the trained models (after saving them from this notebook)\n",
    "def load_models(model_path=\"models/\"):\n",
    "    with open(model_path + \"best_regression_model.pkl\", \"rb\") as f:\n",
    "        reg_model = pickle.load(f)\n",
    "    with open(model_path + \"best_classification_model.pkl\", \"rb\") as f:\n",
    "        cls_model = pickle.load(f)\n",
    "    return reg_model, cls_model\n",
    "\n",
    "# Function to make predictions (similar to our predict_mental_health function)\n",
    "def predict_mental_health(student_data, reg_model, cls_model):\n",
    "    # Convert input to DataFrame\n",
    "    student_df = pd.DataFrame([student_data])\n",
    "    \n",
    "    results = {}\n",
    "    # Get regression prediction\n",
    "    mh_index = reg_model.predict(student_df)[0]\n",
    "    results[\"mental_health_index\"] = mh_index\n",
    "    \n",
    "    # Get classification prediction\n",
    "    mh_category = cls_model.predict(student_df)[0]\n",
    "    category_probs = cls_model.predict_proba(student_df)[0]\n",
    "    results[\"mental_health_category\"] = mh_category\n",
    "    results[\"category_probabilities\"] = {cls_model.classes_[i]: category_probs[i] for i in range(len(cls_model.classes_))}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main function for the Streamlit app\n",
    "def main():\n",
    "    st.title(\"Student Mental Health Prediction Tool\")\n",
    "    st.write(\"This tool helps university counselors assess potential mental health concerns for students.\")\n",
    "    \n",
    "    # Create sidebar for inputs\n",
    "    st.sidebar.header(\"Student Information\")\n",
    "    \n",
    "    # Demographics\n",
    "    st.sidebar.subheader(\"Demographics\")\n",
    "    age = st.sidebar.slider(\"Age\", 17, 30, 20)\n",
    "    gender = st.sidebar.selectbox(\"Gender\", options=[\"Male\", \"Female\", \"Non-binary\"])\n",
    "    year = st.sidebar.selectbox(\"Year of Study\", options=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Academic factors\n",
    "    st.sidebar.subheader(\"Academic Factors\")\n",
    "    gpa = st.sidebar.slider(\"GPA\", 0.0, 4.0, 3.0, 0.1)\n",
    "    course_load = st.sidebar.slider(\"Course Load (# of courses)\", 1, 8, 5)\n",
    "    major = st.sidebar.selectbox(\"Major\", [\"Engineering\", \"Arts\", \"Science\", \"Business\", \"Medicine\"])\n",
    "    \n",
    "    # Behavioral data\n",
    "    st.sidebar.subheader(\"Behavioral Factors\")\n",
    "    sleep = st.sidebar.slider(\"Sleep Hours (daily average)\", 3.0, 10.0, 7.0, 0.5)\n",
    "    exercise = st.sidebar.slider(\"Exercise Hours (weekly)\", 0.0, 20.0, 3.0, 0.5)\n",
    "    social = st.sidebar.slider(\"Social Activity Hours (weekly)\", 0.0, 30.0, 10.0, 1.0)\n",
    "    \n",
    "    # External factors\n",
    "    st.sidebar.subheader(\"External Factors\")\n",
    "    financial_stress = st.sidebar.slider(\"Financial Stress Level\", 0, 4, 2, \n",
    "                                    help=\"0=None, 4=Severe\")\n",
    "    housing = st.sidebar.slider(\"Housing Quality\", 1, 5, 3, \n",
    "                            help=\"1=Poor, 5=Excellent\")\n",
    "    support = st.sidebar.slider(\"Support Network Strength\", 0, 4, 2, \n",
    "                            help=\"0=None, 4=Strong\")\n",
    "    \n",
    "    # Psychological assessments\n",
    "    st.sidebar.subheader(\"Psychological Assessments\")\n",
    "    depression = st.sidebar.slider(\"Depression Score (PHQ-9)\", 0, 27, 5, \n",
    "                               help=\"0-4: Minimal, 5-9: Mild, 10-14: Moderate, 15-19: Moderately Severe, 20-27: Severe\")\n",
    "    anxiety = st.sidebar.slider(\"Anxiety Score (GAD-7)\", 0, 21, 5, \n",
    "                            help=\"0-4: Minimal, 5-9: Mild, 10-14: Moderate, 15-21: Severe\")\n",
    "    stress = st.sidebar.slider(\"Stress Score (PSS)\", 0, 40, 15, \n",
    "                           help=\"0-13: Low, 14-26: Moderate, 27-40: High\")\n",
    "    \n",
    "    # Create a dictionary with all student data\n",
    "    student_data = {\n",
    "        \"age\": age,\n",
    "        \"gender\": gender,\n",
    "        \"year_of_study\": year,\n",
    "        \"gpa\": gpa,\n",
    "        \"course_load\": course_load,\n",
    "        \"major\": major,\n",
    "        \"sleep_hours\": sleep,\n",
    "        \"exercise_hours_per_week\": exercise,\n",
    "        \"social_activity_hours\": social,\n",
    "        \"financial_stress\": financial_stress,\n",
    "        \"housing_quality\": housing,\n",
    "        \"support_network\": support,\n",
    "        \"depression_score\": depression,\n",
    "        \"anxiety_score\": anxiety,\n",
    "        \"stress_score\": stress\n",
    "    }\n",
    "    \n",
    "    # Button to make prediction\n",
    "    if st.sidebar.button(\"Generate Prediction\"):\n",
    "        # Load models (in a real app, you'd do this once at startup)\n",
    "        try:\n",
    "            reg_model, cls_model = load_models()\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction = predict_mental_health(student_data, reg_model, cls_model)\n",
    "            \n",
    "            # Display results\n",
    "            st.header(\"Mental Health Assessment Results\")\n",
    "            \n",
    "            # Display mental health index\n",
    "            mh_index = prediction[\"mental_health_index\"]\n",
    "            st.subheader(f\"Mental Health Index: {mh_index:.1f}/100\")\n",
    "            \n",
    "            # Create a gauge chart for the mental health index\n",
    "            fig, ax = plt.subplots(figsize=(10, 2))\n",
    "            ax.barh([0], [100], color=\"lightgray\", height=0.5)\n",
    "            ax.barh([0], [mh_index], color=plt.cm.RdYlGn(mh_index/100), height=0.5)\n",
    "            ax.set_xlim(0, 100)\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([0, 25, 50, 75, 100])\n",
    "            ax.set_xticklabels([\"0\\nPoor\", \"25\", \"50\\nAverage\", \"75\", \"100\\nExcellent\"])\n",
    "            st.pyplot(fig)\n",
    "            \n",
    "            # Display mental health category\n",
    "            st.subheader(f\"Mental Health Category: {prediction['mental_health_category']}\")\n",
    "            \n",
    "            # Display category probabilities\n",
    "            st.write(\"Probability Breakdown:\")\n",
    "            probs = prediction[\"category_probabilities\"]\n",
    "            for category, prob in probs.items():\n",
    "                st.write(f\"- {category}: {prob:.1%}\")\n",
    "                \n",
    "            # Recommendation section\n",
    "            st.subheader(\"Recommendations\")\n",
    "            if prediction[\"mental_health_category\"] == \"Poor\":\n",
    "                st.error(\"⚠️ This student shows signs of significant mental health concerns. Consider immediate follow-up and referral to psychological services.\")\n",
    "            elif prediction[\"mental_health_category\"] == \"Average\":\n",
    "                st.warning(\"This student shows moderate risk. Regular check-ins and providing resources for support would be beneficial.\")\n",
    "            else:\n",
    "                st.success(\"This student appears to be maintaining good mental health. Continue to provide preventive resources.\")\n",
    "                \n",
    "            # Risk factors analysis\n",
    "            st.subheader(\"Key Risk Factors\")\n",
    "            risk_factors = []\n",
    "            if sleep < 6:\n",
    "                risk_factors.append(\"Insufficient sleep (< 6 hours)\")\n",
    "            if exercise < 1:\n",
    "                risk_factors.append(\"Limited physical activity (< 1 hour weekly)\")\n",
    "            if financial_stress > 2:\n",
    "                risk_factors.append(\"High financial stress\")\n",
    "            if support < 2:\n",
    "                risk_factors.append(\"Limited support network\")\n",
    "            if depression > 9:\n",
    "                risk_factors.append(\"Elevated depression score\")\n",
    "            if anxiety > 9:\n",
    "                risk_factors.append(\"Elevated anxiety score\")\n",
    "            \n",
    "            if risk_factors:\n",
    "                for factor in risk_factors:\n",
    "                    st.write(f\"• {factor}\")\n",
    "            else:\n",
    "                st.write(\"No major risk factors identified.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error making prediction: {str(e)}\")\n",
    "            st.info(\"Note: This is a demo. In production, you would need to train and save the models first.\")\n",
    "    \n",
    "    # Add information about the model\n",
    "    st.sidebar.info(\n",
    "        \"This tool uses machine learning to predict student mental health outcomes. \"\n",
    "        \"It is intended as a screening tool to help identify students who may benefit from support services. \"\n",
    "        \"It should not be used as a diagnostic tool.\"\n",
    "    )\n",
    "    \n",
    "    # Add privacy notice\n",
    "    st.sidebar.warning(\n",
    "        \"PRIVACY NOTICE: Ensure all data is handled in compliance with your institution's privacy policies \"\n",
    "        \"and relevant regulations (FERPA, HIPAA, etc.).\"\n",
    "    )\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "# To run the Streamlit app, save the code to a file named app.py and run:\n",
    "# streamlit run app.py\n",
    "'''\n",
    "\n",
    "# Code to save the trained models\n",
    "def save_models(reg_model, cls_model, model_path=\"models/\"):\n",
    "    \"\"\"Save the trained models to disk for later use in the web application.\"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # Save regression model\n",
    "    with open(model_path + \"best_regression_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(reg_model, f)\n",
    "    \n",
    "    # Save classification model\n",
    "    with open(model_path + \"best_classification_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(cls_model, f)\n",
    "    \n",
    "    print(f\"Models saved successfully to {model_path}\")\n",
    "\n",
    "# Uncomment this to save your best models\n",
    "# save_models(best_reg_model, best_cls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b10db",
   "metadata": {},
   "source": [
    "## 12. Deep Learning Approach\n",
    "\n",
    "Traditional machine learning models like K-NN, Linear Regression, and SVM have provided good results, but deep learning approaches might capture more complex patterns in the data, especially with larger datasets. Let's implement a simple neural network model for our mental health prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement deep learning, we'll need TensorFlow/Keras\n",
    "# Uncomment to install if needed\n",
    "# !pip install tensorflow\n",
    "\n",
    "'''\n",
    "Example neural network implementation for mental health prediction:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare data for deep learning\n",
    "def prepare_data_for_nn(X_train, X_test, y_train, y_test):\n",
    "    # We need to preprocess the data differently for neural networks\n",
    "    categorical_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    numerical_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numerical_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    X_train_nn = preprocessor.fit_transform(X_train)\n",
    "    X_test_nn = preprocessor.transform(X_test)\n",
    "    \n",
    "    return X_train_nn, X_test_nn, preprocessor\n",
    "\n",
    "# 2. Create and train a neural network for regression\n",
    "def build_regression_nn(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)  # No activation for regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"mean_squared_error\"\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 3. Create and train a neural network for classification\n",
    "def build_classification_nn(input_dim, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare data for neural network\n",
    "X_train_nn, X_test_nn, preprocessor = prepare_data_for_nn(X_train, X_test, y_train_reg, y_test_reg)\n",
    "\n",
    "# Get dimensions\n",
    "input_dim = X_train_nn.shape[1]\n",
    "num_classes = len(np.unique(y_train_cls))\n",
    "\n",
    "# Create regression model\n",
    "nn_reg_model = build_regression_nn(input_dim)\n",
    "\n",
    "# Create classification model\n",
    "nn_cls_model = build_classification_nn(input_dim, num_classes)\n",
    "\n",
    "# Train regression model\n",
    "print(\"Training Neural Network Regression Model...\")\n",
    "history_reg = nn_reg_model.fit(\n",
    "    X_train_nn, y_train_reg,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train classification model\n",
    "print(\"Training Neural Network Classification Model...\")\n",
    "# Convert categorical labels to numbers (0, 1, 2, ...)\n",
    "y_train_cls_numeric = pd.Categorical(y_train_cls).codes\n",
    "y_test_cls_numeric = pd.Categorical(y_test_cls).codes\n",
    "\n",
    "history_cls = nn_cls_model.fit(\n",
    "    X_train_nn, y_train_cls_numeric,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate regression model\n",
    "y_pred_reg_nn = nn_reg_model.predict(X_test_nn)\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_nn))\n",
    "r2_nn = r2_score(y_test_reg, y_pred_reg_nn)\n",
    "\n",
    "print(f\"Neural Network Regression Model - RMSE: {rmse_nn:.4f}, R²: {r2_nn:.4f}\")\n",
    "\n",
    "# Evaluate classification model\n",
    "y_pred_cls_nn = nn_cls_model.predict(X_test_nn)\n",
    "y_pred_cls_nn_classes = np.argmax(y_pred_cls_nn, axis=1)\n",
    "accuracy_nn = accuracy_score(y_test_cls_numeric, y_pred_cls_nn_classes)\n",
    "\n",
    "print(f\"Neural Network Classification Model - Accuracy: {accuracy_nn:.4f}\")\n",
    "\n",
    "# Plot training history for regression model\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_reg.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history_reg.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Neural Network Regression Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training history for classification model\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cls.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history_cls.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Neural Network Classification Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add neural network results to our comparison\n",
    "nn_reg_results = {\n",
    "    \"Model\": \"Neural Network\",\n",
    "    \"RMSE\": rmse_nn,\n",
    "    \"R²\": r2_nn\n",
    "}\n",
    "\n",
    "nn_cls_results = {\n",
    "    \"Model\": \"Neural Network\",\n",
    "    \"Accuracy\": accuracy_nn\n",
    "}\n",
    "\n",
    "# Update our results dataframes\n",
    "all_reg_results_with_nn = pd.DataFrame(reg_results + [rf_reg_results, nn_reg_results]).set_index(\"Model\")\n",
    "all_cls_results_with_nn = pd.DataFrame([{r[\"Model\"]: r[\"Accuracy\"] for r in cls_results}, \n",
    "                                     {\"Random Forest\": rf_cls_results[\"Accuracy\"],\n",
    "                                      \"Neural Network\": nn_cls_results[\"Accuracy\"]}]).T\n",
    "all_cls_results_with_nn.columns = [\"Accuracy\"]\n",
    "\n",
    "# Plot final comparison with all models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=all_reg_results_with_nn.index, y=all_reg_results_with_nn[\"RMSE\"])\n",
    "plt.title(\"All Regression Models: RMSE Comparison (Lower is Better)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=all_cls_results_with_nn.index, y=all_cls_results_with_nn[\"Accuracy\"])\n",
    "plt.title(\"All Classification Models: Accuracy Comparison (Higher is Better)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "print(\"Deep learning models require TensorFlow/Keras. To run this section, uncomment the installation command and the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908f75d",
   "metadata": {},
   "source": [
    "## 13. Ethics and Responsible AI for Mental Health Applications\n",
    "\n",
    "Developing AI systems for mental health prediction introduces important ethical considerations that must be addressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcee04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Ethical Considerations for Mental Health AI\n",
    "\n",
    "1. **Privacy and Confidentiality**\n",
    "   - Mental health data is highly sensitive and requires strict privacy protections\n",
    "   - All data should be anonymized, encrypted, and securely stored\n",
    "   - Access should be limited to authorized personnel only\n",
    "\n",
    "2. **Informed Consent**\n",
    "   - Students must fully understand how their data will be used\n",
    "   - Clear explanation of the AI system's purpose, limitations, and risks\n",
    "   - Right to withdraw consent and have data deleted\n",
    "\n",
    "3. **Bias and Fairness**\n",
    "   - Ensure the model doesn't discriminate based on race, gender, socioeconomic status, etc.\n",
    "   - Regularly audit predictions for demographic disparities\n",
    "   - Use diverse training data to minimize inherent biases\n",
    "\n",
    "4. **Transparency and Explainability**\n",
    "   - Stakeholders should understand how the system makes predictions\n",
    "   - Use explainable AI techniques (like SHAP values shown earlier)\n",
    "   - Documenting model limitations and uncertainty\n",
    "\n",
    "5. **Human Oversight and Intervention**\n",
    "   - AI should supplement, not replace, professional judgment\n",
    "   - Clear protocols for when human review is necessary\n",
    "   - Mental health professionals must remain the decision-makers\n",
    "\n",
    "6. **Validation and Accuracy**\n",
    "   - Rigorous clinical validation before deployment\n",
    "   - Regular monitoring of model performance\n",
    "   - Clear communication of confidence levels in predictions\n",
    "\n",
    "7. **Supporting Positive Interventions**\n",
    "   - Predictions should lead to helpful interventions, not stigmatization\n",
    "   - Focus on early support and preventive measures\n",
    "   - Provide resources alongside predictions\n",
    "\n",
    "8. **Regulatory Compliance**\n",
    "   - Adhere to relevant regulations (HIPAA, FERPA, GDPR, etc.)\n",
    "   - Follow institutional policies for student data\n",
    "   - Regular ethics reviews and updates\n",
    "'''\n",
    "\n",
    "# Example code for fairness analysis\n",
    "\n",
    "def analyze_model_fairness(model, X_test, y_test, sensitive_attribute='gender'):\n",
    "    \"\"\"Analyze whether the model shows bias across a sensitive attribute.\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Group performance by the sensitive attribute\n",
    "    results = []\n",
    "    for group in X_test[sensitive_attribute].unique():\n",
    "        mask = X_test[sensitive_attribute] == group\n",
    "        group_X = X_test[mask]\n",
    "        group_y_true = y_test[mask]\n",
    "        group_y_pred = model.predict(group_X)\n",
    "        \n",
    "        if isinstance(y_test, pd.Series) and y_test.dtype == 'category':\n",
    "            # For classification\n",
    "            accuracy = accuracy_score(group_y_true, group_y_pred)\n",
    "            results.append({\n",
    "                sensitive_attribute: group,\n",
    "                'accuracy': accuracy,\n",
    "                'count': len(group_X)\n",
    "            })\n",
    "        else:\n",
    "            # For regression\n",
    "            rmse = np.sqrt(mean_squared_error(group_y_true, group_y_pred))\n",
    "            r2 = r2_score(group_y_true, group_y_pred)\n",
    "            results.append({\n",
    "                sensitive_attribute: group,\n",
    "                'RMSE': rmse,\n",
    "                'R²': r2,\n",
    "                'count': len(group_X)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage (uncomment to use)\n",
    "'''\n",
    "# For classification model\n",
    "fairness_cls = analyze_model_fairness(best_cls_model, X_test, y_test_cls)\n",
    "print(\"Classification model fairness analysis:\")\n",
    "print(fairness_cls)\n",
    "\n",
    "# For regression model\n",
    "fairness_reg = analyze_model_fairness(best_reg_model, X_test, y_test_reg)\n",
    "print(\"\\nRegression model fairness analysis:\")\n",
    "print(fairness_reg)\n",
    "\n",
    "# Visualize fairness comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "if 'accuracy' in fairness_cls.columns:\n",
    "    sns.barplot(x=sensitive_attribute, y='accuracy', data=fairness_cls)\n",
    "    plt.title(f'Model Accuracy Across {sensitive_attribute.capitalize()} Groups')\n",
    "    plt.ylabel('Accuracy')\n",
    "else:\n",
    "    sns.barplot(x=sensitive_attribute, y='RMSE', data=fairness_reg)\n",
    "    plt.title(f'Model RMSE Across {sensitive_attribute.capitalize()} Groups')\n",
    "    plt.ylabel('RMSE (Lower is Better)')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "print(\"Ethical considerations are crucial when developing AI for mental health applications.\")\n",
    "print(\"The code above provides a framework for analyzing model fairness across sensitive attributes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6ddb7",
   "metadata": {},
   "source": [
    "## 14. Final Steps and Real-World Implementation\n",
    "\n",
    "To move this project from a proof-of-concept to a real-world implementation, here is a roadmap of next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. **Data Collection and IRB Approval**\n",
    "   - Design a data collection protocol\n",
    "   - Secure IRB/Ethics approval\n",
    "   - Establish data governance policies\n",
    "   - Begin collecting real student data\n",
    "\n",
    "2. **Model Refinement**\n",
    "   - Retrain models with real data\n",
    "   - Optimize hyperparameters for production\n",
    "   - Perform cross-validation with multiple cohorts\n",
    "   - Add model versioning and tracking\n",
    "\n",
    "3. **System Implementation**\n",
    "   - Develop a production-ready prediction API\n",
    "   - Build a secure web interface for counselors\n",
    "   - Implement authentication and access controls\n",
    "   - Establish data backup and recovery procedures\n",
    "\n",
    "4. **Validation Study**\n",
    "   - Compare model predictions with expert assessments\n",
    "   - Measure predictive accuracy over time\n",
    "   - Gather feedback from mental health professionals\n",
    "   - Refine model based on findings\n",
    "\n",
    "5. **Integration with University Systems**\n",
    "   - Connect with existing student support systems\n",
    "   - Establish referral workflows\n",
    "   - Create documentation and training materials\n",
    "   - Develop standard operating procedures\n",
    "\n",
    "6. **Monitoring and Maintenance**\n",
    "   - Set up continuous model performance monitoring\n",
    "   - Schedule regular model retraining\n",
    "   - Track intervention outcomes\n",
    "   - Conduct periodic fairness audits\n",
    "\n",
    "7. **Expansion and Research**\n",
    "   - Consider additional data sources (academic performance, etc.)\n",
    "   - Test alternative prediction approaches\n",
    "   - Publish findings to advance the field\n",
    "   - Share anonymized insights with the educational community\n",
    "'''\n",
    "\n",
    "# Sample implementation timeline\n",
    "implementation_timeline = pd.DataFrame({\n",
    "    'Phase': ['Planning & Ethics', 'Data Collection', 'Model Development', 'Validation', 'Deployment', 'Monitoring'],\n",
    "    'Duration': ['2 months', '4 months', '3 months', '2 months', '1 month', 'Ongoing'],\n",
    "    'Key Activities': [\n",
    "        'IRB approval, protocol design', \n",
    "        'Survey implementation, data gathering', \n",
    "        'Model training and optimization', \n",
    "        'Expert validation, user testing',\n",
    "        'Web app launch, user training',\n",
    "        'Performance tracking, model updates'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Implementation Timeline:\")\n",
    "print(implementation_timeline)\n",
    "\n",
    "print(\"\\nThank you for exploring this multimodal machine learning approach to mental health prediction.\")\n",
    "print(\"With real data and careful implementation, this system could help university counseling services\")\n",
    "print(\"identify students who might benefit from early mental health support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41280bd7",
   "metadata": {},
   "source": [
    "## 15. Summary and Conclusions\n",
    "\n",
    "In this comprehensive notebook, we've developed a multimodal machine learning approach to predict mental health outcomes for university students. We've covered:\n",
    "\n",
    "1. **Data Processing**: Handling both numerical and categorical data through preprocessing pipelines\n",
    "2. **Model Implementation**: Using K-NN, Linear/Logistic Regression, SVM, Random Forest, and Neural Networks\n",
    "3. **Model Evaluation**: Comparing performance metrics across different algorithms\n",
    "4. **Explainable AI**: Using SHAP values to interpret model predictions\n",
    "5. **Deployment Strategy**: Creating a web application for university counselors\n",
    "6. **Ethical Considerations**: Addressing fairness, privacy, and responsible use\n",
    "7. **Next Steps**: Planning for real-world implementation\n",
    "\n",
    "The models showed promising results even with synthetic data. With real student data and proper validation, this approach could become a valuable tool for early identification of mental health concerns among university students, ultimately supporting timely intervention and better student wellbeing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
